{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55272bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Aaron Hertner\n",
    "#Version: Python Base 3.8\n",
    "#Souce Material: Provided by Laura Mai, 'Users/Laura.Mai/neural_network/notebooks/LSTM.ipynb'\n",
    "#Purpose: To further experiment on the established optimal ML model and find a solution which will make the model dataset agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34455414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from gensim import corpora\n",
    "from loglizer.models import PCA, InvariantsMiner, LogClustering, IsolationForest\n",
    "from loglizer import dataloader, preprocessing\n",
    "from nltk import everygrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfdae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_FILENAME = '../reports/Thunderbird_unsupervised.csv'\n",
    "\n",
    "#LOG_NAME = \"HDFS/1\" # Thunderbird, BGL, HDFS/1, OpenStack, or HDFS.npz\n",
    "LOG_NAME=\"HDFS/1\"\n",
    "\n",
    "#Array of Model Names\n",
    "RUN_MODELS = ['PCA',\n",
    "              'LogClustering', \n",
    "              'IsolationForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c13488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate case for different file type\n",
    "if LOG_NAME == \"HDFS.npz\":\n",
    "    struct_log = '../data/HDFS.npz'  # The benchmark dataset\n",
    "    (x_tr, y_train), (x_te, y_test) = dataloader.load_HDFS(struct_log,\n",
    "                                                           window='session',\n",
    "                                                           train_ratio=0.5,\n",
    "                                                           split_type='uniform')\n",
    "    for i, x in enumerate(x_tr):\n",
    "        x_tr[i] = \" \".join(x)\n",
    "    for i, x in enumerate(x_te):\n",
    "        x_te[i] = \" \".join(x)\n",
    "\n",
    "#case for normal .csv file type\n",
    "else:        \n",
    "    if LOG_NAME == \"HDFS/1\":\n",
    "        processed_filename = '../LAD/data/processed/HDFS/1/HDFS.csv'\n",
    "    elif LOG_NAME == \"BGL\":\n",
    "        processed_filename = '../LAD/data/processed/BGL/BGL.csv'\n",
    "    elif LOG_NAME == \"Thunderbird\":\n",
    "        processed_filename = '../LAD/data/processed/Thunderbird/Thunderbird*.csv'\n",
    "    else:\n",
    "        raise Exception(LOG_NAME + \" is not an option\")\n",
    "        \n",
    "    #process csv\n",
    "    data_df = dd.read_csv(processed_filename)\n",
    "    \n",
    "    \n",
    "    #create training and testing sets on loaded data\n",
    "    x_tr, x_te, y_train, y_test = train_test_split(\n",
    "        data_df[\"EventSequence\"].values.compute(),\n",
    "        data_df[\"Label\"].values.compute(),\n",
    "        test_size=0.5,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8549215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Anomalies: 288250\n",
      "Total Number of Normal Entries: 10887379\n",
      "\n",
      "Anomalies:      0.03\n",
      "Normal Entries: 0.97\n"
     ]
    }
   ],
   "source": [
    "anomaly = 0\n",
    "normal = 0\n",
    "\n",
    "#NOTE: \n",
    "\n",
    "#count normal entries in training set\n",
    "for x in x_tr[y_train==0]:\n",
    "    normal +=  len(ast.literal_eval(x))\n",
    "    \n",
    "#count normal entries in testing set\n",
    "for x in x_te[y_test==0]:\n",
    "    normal +=  len(ast.literal_eval(x))\n",
    "    \n",
    "#count anomalies in training set\n",
    "for x in x_tr[y_train==1]:\n",
    "    anomaly +=  len(ast.literal_eval(x))\n",
    "    \n",
    "#count anomalies in testing set\n",
    "for x in x_te[y_test==1]:\n",
    "    anomaly +=  len(ast.literal_eval(x))\n",
    "    \n",
    "print(f'Total Number of Anomalies: {anomaly}')\n",
    "print(f'Total Number of Normal Entries: {normal}\\n')\n",
    "print('Anomalies:      %.2f' %(anomaly / (anomaly+normal)))\n",
    "print('Normal Entries: %.2f' %(normal / (anomaly+normal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24be1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "#directory for processed data\n",
    "PROCESSED_DIR = \"../LAD/data/processed\"\n",
    "\n",
    "#select log we wish to use\n",
    "LOG_NAME = \"HDFS/1\"\n",
    "\n",
    "#set window size\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "#grab dictionary for the dataset\n",
    "dictionary = corpora.Dictionary().load(os.path.join(PROCESSED_DIR, LOG_NAME, \"sequential\", \"dictionary.pkl\"))\n",
    "\n",
    "#dictionary size\n",
    "dict_len = len(dictionary.token2id)\n",
    "\n",
    "print(dict_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2400e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID        EventSequence  Prediction  Label   0   1   2   3   4\n",
      "0   0  [10, 0, 10, 10, 11]          11      0  10   0  10  10  11\n",
      "1   0  [0, 10, 10, 11, 11]           3      0   0  10  10  11  11\n",
      "2   0  [10, 10, 11, 11, 3]           3      0  10  10  11  11   3\n",
      "3   0   [10, 11, 11, 3, 3]          11      0  10  11  11   3   3\n",
      "4   0   [11, 11, 3, 3, 11]           3      0  11  11   3   3  11\n"
     ]
    }
   ],
   "source": [
    "#grab file names for processed log\n",
    "file_names = glob.glob(os.path.join(PROCESSED_DIR, LOG_NAME, \"sequential\", \"00.csv\"))\n",
    "\n",
    "#if there is only one matching file then we process it's .csv again\n",
    "if len(file_names)==1:\n",
    "    data_df = pd.read_csv(file_names[0], index_col=None, header=0)\n",
    "    \n",
    "#if there are more than one matching files then process all of them\n",
    "else:\n",
    "    li = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        print(file_name)\n",
    "        df = pd.read_csv(file_name, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    data_df = pd.concat(li, axis=0, ignore_index=True) #create a dataframe of all .csv files\n",
    "    df = None\n",
    "    li = None\n",
    "    \n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f82eb7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTRAIN=======================================\n",
      "[[ 3. 11.  3. 11.  3.]\n",
      " [11.  3.  6.  6.  6.]\n",
      " [ 3.  6.  6.  6. 12.]\n",
      " ...\n",
      " [ 3.  6.  6. 12. 12.]\n",
      " [16.  4. 14. 14. 12.]\n",
      " [ 3. 11.  3. 11.  3.]]\n",
      "\n",
      "YTRAIN=======================================\n",
      "[[ 6]\n",
      " [11]\n",
      " [12]\n",
      " ...\n",
      " [12]\n",
      " [12]\n",
      " [ 6]]\n",
      "\n",
      "ID-Train=====================================\n",
      "[366430 273746 552359 ... 439941 112801 341624]\n",
      "\n",
      "LABEL-Train==================================\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#split our resultant dataframe into training and testing sets for X, Y, ID, and Label\n",
    "x_train, x_test, y_train, y_test, id_train, id_test, label_train, label_test = train_test_split(\n",
    "    data_df[[\"0\", \"1\", \"2\", \"3\", \"4\"]].to_numpy(),\n",
    "    data_df[\"Prediction\"].to_numpy(),\n",
    "    data_df[\"ID\"].to_numpy(),\n",
    "    data_df[\"Label\"].to_numpy(),\n",
    "    test_size=0.5,\n",
    "    random_state=5,\n",
    "    shuffle=True)\n",
    "\n",
    "x_train = np.array(list(x for x in x_train), dtype=np.float64)\n",
    "y_train = np.array(list([x] for x in y_train), dtype=np.int64)\n",
    "x_test = np.array(list(x for x in x_test), dtype=np.float64)\n",
    "y_test = np.array(list([x] for x in y_test), dtype=np.float64)\n",
    "\n",
    "print('XTRAIN=======================================')\n",
    "print(x_train)\n",
    "print('\\nYTRAIN=======================================')\n",
    "print(y_train)\n",
    "print('\\nID-Train=====================================')\n",
    "print(id_train)\n",
    "print('\\nLABEL-Train==================================')\n",
    "print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "334498f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "101203/101203 [==============================] - 644s 6ms/step - loss: 2.8412 - sparse_top_k_categorical_accuracy: 0.4134 - val_loss: 2.8291 - val_sparse_top_k_categorical_accuracy: 0.4568\n",
      "Epoch 2/10\n",
      "101203/101203 [==============================] - 640s 6ms/step - loss: 2.8294 - sparse_top_k_categorical_accuracy: 0.4824 - val_loss: 2.8285 - val_sparse_top_k_categorical_accuracy: 0.5629\n",
      "Epoch 3/10\n",
      "101203/101203 [==============================] - 640s 6ms/step - loss: 2.8290 - sparse_top_k_categorical_accuracy: 0.5467 - val_loss: 2.8282 - val_sparse_top_k_categorical_accuracy: 0.6602\n",
      "Epoch 4/10\n",
      "101203/101203 [==============================] - 638s 6ms/step - loss: 2.8285 - sparse_top_k_categorical_accuracy: 0.6013 - val_loss: 2.8275 - val_sparse_top_k_categorical_accuracy: 0.6898\n",
      "Epoch 5/10\n",
      "101203/101203 [==============================] - 660s 7ms/step - loss: 2.8280 - sparse_top_k_categorical_accuracy: 0.6285 - val_loss: 2.8273 - val_sparse_top_k_categorical_accuracy: 0.6977\n",
      "Epoch 6/10\n",
      "101203/101203 [==============================] - 629s 6ms/step - loss: 2.8279 - sparse_top_k_categorical_accuracy: 0.6406 - val_loss: 2.8274 - val_sparse_top_k_categorical_accuracy: 0.4066\n",
      "Epoch 7/10\n",
      "101203/101203 [==============================] - 638s 6ms/step - loss: 2.8279 - sparse_top_k_categorical_accuracy: 0.6516 - val_loss: 2.8272 - val_sparse_top_k_categorical_accuracy: 0.7112\n",
      "Epoch 8/10\n",
      "101203/101203 [==============================] - 642s 6ms/step - loss: 2.8278 - sparse_top_k_categorical_accuracy: 0.6599 - val_loss: 2.8273 - val_sparse_top_k_categorical_accuracy: 0.6772\n",
      "Epoch 9/10\n",
      "101203/101203 [==============================] - 638s 6ms/step - loss: 2.8278 - sparse_top_k_categorical_accuracy: 0.6649 - val_loss: 2.8272 - val_sparse_top_k_categorical_accuracy: 0.7124\n",
      "Epoch 10/10\n",
      "101203/101203 [==============================] - 658s 7ms/step - loss: 2.8277 - sparse_top_k_categorical_accuracy: 0.6706 - val_loss: 2.8272 - val_sparse_top_k_categorical_accuracy: 0.5803\n",
      "Model: \"lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 5, 16)             1152      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 16)             2112      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 5, 16)             2112      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 72)                1224      \n",
      "=================================================================\n",
      "Total params: 8,712\n",
      "Trainable params: 8,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#input layer\n",
    "main_input = keras.layers.Input(shape=(WINDOW_SIZE,))\n",
    "\n",
    "#initial embedding layer\n",
    "x = keras.layers.Embedding(dict_len, 16)(main_input)\n",
    "\n",
    "#LSTM Layers\n",
    "x = keras.layers.LSTM(16, return_sequences=True)(x)\n",
    "x = keras.layers.LSTM(16, return_sequences=True)(x)\n",
    "x = keras.layers.LSTM(16)(x) \n",
    "\n",
    "#output layer\n",
    "output = keras.layers.Dense(dict_len, activation=\"softmax\")(x)\n",
    "\n",
    "#create the model we wish to use using keras' model class\n",
    "model = keras.Model(main_input, output, name=\"lstm\")\n",
    "\n",
    "#defining a loss function for compilation\n",
    "def nll1(dict_len):\n",
    "    def loss(y_true, y_pred):\n",
    "        _y_true = tf.one_hot(y_true, dict_len)\n",
    "        return K.sum(K.binary_crossentropy(_y_true, y_pred), axis=-1)\n",
    "    return loss\n",
    "    \n",
    "#configure the model, preparing it for training\n",
    "model.compile(\n",
    "    loss=nll1(dict_len),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), #Originally 0.0001\n",
    "    metrics=[keras.metrics.SparseTopKCategoricalAccuracy(k=math.ceil(dict_len*0.02))]\n",
    ") \n",
    "\n",
    "#train the model - we use only non-anomalous data\n",
    "history = model.fit(x_train[label_train == 0],\n",
    "          y_train[label_train == 0],\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          shuffle=True,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2dd54856",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "ids = id_test\n",
    "labels = label_test\n",
    "actuals = y_test\n",
    "data_test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0032c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.3459276e-05 3.5008983e-04 3.0810182e-04 ... 4.1705989e-03\n",
      "  2.3877865e-08 1.7389617e-08]\n",
      " [5.0577157e-05 2.8203567e-04 2.4999716e-04 ... 3.9342884e-03\n",
      "  1.0973323e-08 1.0800107e-08]\n",
      " [5.9979491e-05 3.2716588e-04 3.0432953e-04 ... 4.4592731e-03\n",
      "  2.1461810e-08 1.8981208e-08]\n",
      " ...\n",
      " [6.0062575e-05 3.2507756e-04 2.7875518e-04 ... 3.9410102e-03\n",
      "  1.8114214e-08 1.3880096e-08]\n",
      " [7.3459276e-05 3.5008983e-04 3.0810182e-04 ... 4.1705989e-03\n",
      "  2.3877865e-08 1.7389617e-08]\n",
      " [7.3459276e-05 3.5008983e-04 3.0810182e-04 ... 4.1705989e-03\n",
      "  2.3877865e-08 1.7389617e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "158200c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65214  1294]\n",
      " [  589  3523]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     66508\n",
      "           1       0.73      0.86      0.79      4112\n",
      "\n",
      "    accuracy                           0.97     70620\n",
      "   macro avg       0.86      0.92      0.89     70620\n",
      "weighted avg       0.98      0.97      0.97     70620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "TOP_X = math.ceil(dict_len*0.1)\n",
    "data_dict = OrderedDict()\n",
    "\n",
    "for i, _id in enumerate(ids):\n",
    "    if not _id in data_dict:\n",
    "        data_dict[_id] = [0, labels[i], 0]\n",
    "    \n",
    "    actual = actuals[i][0]\n",
    "    tmp = heapq.nlargest(TOP_X, range(len(preds[i])), preds[i].__getitem__)\n",
    "    \n",
    "    if not np.in1d(actual, tmp)[0]:\n",
    "        data_dict[_id][0] += 1\n",
    "    data_dict[_id][2] += 1\n",
    "        \n",
    "pred_df = pd.DataFrame(list(data_dict.items()), columns=[\"id\", \"diff\"])\n",
    "\n",
    "pred_df[\"label\"] = pred_df[\"diff\"].apply(lambda x: x[1])\n",
    "pred_df[\"total\"] = pred_df[\"diff\"].apply(lambda x: x[2])\n",
    "pred_df[\"diff\"] = pred_df[\"diff\"].apply(lambda x: x[0])\n",
    "pred_df[\"pred\"] = pred_df[\"diff\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "g = pred_df.loc[pred_df.total >10]\n",
    "\n",
    "print(confusion_matrix(g[\"label\"], g[\"pred\"]))\n",
    "print(classification_report(g[\"label\"], g[\"pred\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e24ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
