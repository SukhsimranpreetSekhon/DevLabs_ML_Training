{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ad0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Aaron Hertner\n",
    "#Version: Python 3.8 Base\n",
    "#Purpose: To develop a new model on our existing log anomaly data and integrate it in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31df1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 14:03:11.044229: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-10-19 14:03:11.044270: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydot\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from gensim import corpora\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from loglizer.models import PCA, DecisionTree, LogClustering\n",
    "from loglizer import dataloader, preprocessing\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cdd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     duration  http_status_code  \\\n",
      "0       0.145               201   \n",
      "1       0.134               201   \n",
      "2       0.083               201   \n",
      "3       0.045               201   \n",
      "4       0.135               201   \n",
      "..        ...               ...   \n",
      "995     0.051               201   \n",
      "996     0.042               201   \n",
      "997     0.215               201   \n",
      "998     0.046               201   \n",
      "999     0.035               201   \n",
      "\n",
      "                                               comment  Anomalous  \n",
      "0    One of the other reviewers has mentioned that ...          0  \n",
      "1    A wonderful little production. <br /><br />The...          0  \n",
      "2    I thought this was a wonderful way to spend ti...          0  \n",
      "3    Basically there\\'s a family where a little boy...          0  \n",
      "4    Petter Mattei\\'s \\\\\"Love in the Time of Money\\...          0  \n",
      "..                                                 ...        ...  \n",
      "995  <area onfocusout=alert(1) tabindex=1 id=x></ar...          1  \n",
      "996    <textarea onpointermove=alert(1)>XSS</textarea>          1  \n",
      "997                                     document.write          1  \n",
      "998           <area onpointerenter=alert(1)>XSS</area>          1  \n",
      "999  <dl draggable=\\\\\"false\\\\\" ondrag=\\\\\"alert(0)\\\\...          1  \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_log_data.csv')\n",
    "df_comments = df['comment']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d45bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes\n",
    "normal_df = pd.DataFrame()\n",
    "anomalous_df = pd.DataFrame()\n",
    "\n",
    "#arrays\n",
    "normal = []\n",
    "anomalous = []\n",
    "duration_a = []\n",
    "duration_n = []\n",
    "sc_a = []\n",
    "sc_n = []\n",
    "\n",
    "for i in range (len(df['comment'])):\n",
    "    if df['Anomalous'][i] == 1:\n",
    "        anomalous.append(df['comment'][i])\n",
    "        duration_a.append(df['duration'][i])\n",
    "        sc_a.append(df['http_status_code'][i])\n",
    "    else:\n",
    "        normal.append(df['comment'][i])\n",
    "        duration_n.append(df['duration'][i])\n",
    "        sc_n.append(df['http_status_code'][i])\n",
    "        \n",
    "anomalous_df[\"comment\"] = anomalous\n",
    "anomalous_df['http_code'] = sc_a\n",
    "anomalous_df['duration'] = duration_a\n",
    "\n",
    "normal_df[\"comment\"] = normal\n",
    "normal_df['http_code'] = sc_n\n",
    "normal_df['duration'] = duration_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9981818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               comment  http_code  duration  \\\n",
      "0    x\\' and 1 =  ( select count ( * )  from tabnam...        201     0.497   \n",
      "1                        x\\' and members.email is NULL        201     0.036   \n",
      "2                        x\\' or full_name like \\'%bob%        201     0.036   \n",
      "3    \\' AND 1 = utl_inaddr.get_host_address  (  (  ...        201     0.035   \n",
      "4    \\' AND 1 = utl_inaddr.get_host_address  (  (  ...        201     0.056   \n",
      "..                                                 ...        ...       ...   \n",
      "195  <area onfocusout=alert(1) tabindex=1 id=x></ar...        201     0.051   \n",
      "196    <textarea onpointermove=alert(1)>XSS</textarea>        201     0.042   \n",
      "197                                     document.write        201     0.215   \n",
      "198           <area onpointerenter=alert(1)>XSS</area>        201     0.046   \n",
      "199  <dl draggable=\\\\\"false\\\\\" ondrag=\\\\\"alert(0)\\\\...        201     0.035   \n",
      "\n",
      "     encoding  \n",
      "0           5  \n",
      "1           5  \n",
      "2           4  \n",
      "3          10  \n",
      "4           9  \n",
      "..        ...  \n",
      "195         8  \n",
      "196         5  \n",
      "197         2  \n",
      "198         5  \n",
      "199         7  \n",
      "\n",
      "[200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_counts(arr):\n",
    "    count_array = []\n",
    "\n",
    "    #vectorize the comments\n",
    "    vct = CountVectorizer(lowercase=True)\n",
    "    vct_cmt = vct.fit_transform(arr)\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    comment_array = vct_cmt.toarray()\n",
    "\n",
    "    #sum the counts within each array for each comment\n",
    "    for i in range(len(comment_array)):\n",
    "        sum = 0\n",
    "        for num in comment_array[i]:\n",
    "            sum = sum + num\n",
    "        count_array.append(sum)\n",
    "    \n",
    "    return count_array\n",
    "\n",
    "anom_counts = create_counts(anomalous)\n",
    "anomalous_df['encoding'] = anom_counts\n",
    "    \n",
    "normal_counts = create_counts(normal)\n",
    "normal_df['encoding'] = normal_counts\n",
    "\n",
    "print(anomalous_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5cdc2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     http_code  duration  encoding\n",
      "0          201     0.497         5\n",
      "1          201     0.036         5\n",
      "2          201     0.036         4\n",
      "3          201     0.035        10\n",
      "4          201     0.056         9\n",
      "..         ...       ...       ...\n",
      "195        201     0.051         8\n",
      "196        201     0.042         5\n",
      "197        201     0.215         2\n",
      "198        201     0.046         5\n",
      "199        201     0.035         7\n",
      "\n",
      "[200 rows x 3 columns]\n",
      "     http_code  duration  encoding\n",
      "0          201     0.145       307\n",
      "1          201     0.134       160\n",
      "2          201     0.083       158\n",
      "3          201     0.045       126\n",
      "4          201     0.135       227\n",
      "..         ...       ...       ...\n",
      "795        201     0.053       444\n",
      "796        201     0.033       124\n",
      "797        201     0.240       176\n",
      "798        201     0.034       164\n",
      "799        201     0.069       272\n",
      "\n",
      "[800 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "anomalous_df = anomalous_df.drop(['comment'], axis=1)\n",
    "normal_df = normal_df.drop(['comment'], axis=1)\n",
    "\n",
    "print(anomalous_df)\n",
    "print(normal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325f01df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     http_code  duration  encoding  target\n",
      "0          0.0  0.752575  0.460037      -1\n",
      "1          0.0  0.639662 -0.408933      -1\n",
      "2          0.0  0.116159 -0.420756      -1\n",
      "3          0.0 -0.273904 -0.609920      -1\n",
      "4          0.0  0.649927 -0.012872      -1\n",
      "..         ...       ...       ...     ...\n",
      "795        0.0 -0.191783  1.269894      -1\n",
      "796        0.0 -0.397080 -0.621742      -1\n",
      "797        0.0  1.727728 -0.314352      -1\n",
      "798        0.0 -0.386814 -0.385288      -1\n",
      "799        0.0 -0.027548  0.253139      -1\n",
      "\n",
      "[800 rows x 4 columns]\n",
      "     http_code  duration  encoding  target\n",
      "0          0.0  2.096815 -0.773535       1\n",
      "1          0.0 -0.385482 -0.773535       1\n",
      "2          0.0 -0.385482 -0.993915       1\n",
      "3          0.0 -0.390867  0.328367       1\n",
      "4          0.0 -0.277791  0.107986       1\n",
      "..         ...       ...       ...     ...\n",
      "195        0.0 -0.304713 -0.112394       1\n",
      "196        0.0 -0.353174 -0.773535       1\n",
      "197        0.0  0.578360 -1.434676       1\n",
      "198        0.0 -0.331638 -0.773535       1\n",
      "199        0.0 -0.390867 -0.332774       1\n",
      "\n",
      "[200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "normal_sc = sc.fit_transform(normal_df)\n",
    "normal_scaled_df = pd.DataFrame(normal_sc, columns=normal_df.columns)\n",
    "normal_scaled_df['target'] = -1\n",
    "print(normal_scaled_df)\n",
    "\n",
    "anom_sc = sc.fit_transform(anomalous_df)\n",
    "anom_scaled_df = pd.DataFrame(anom_sc, columns=anomalous_df.columns)\n",
    "anom_scaled_df['target'] = 1\n",
    "print(anom_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd02376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     http_code  duration  encoding\n",
      "96         0.0 -0.326252 -0.112394\n",
      "262        0.0 -0.243108  1.482703\n",
      "980        0.0  0.075100  0.146735\n",
      "361        0.0 -0.376550 -0.840463\n",
      "328        0.0 -0.099401 -0.533072\n",
      "..         ...       ...       ...\n",
      "716        0.0 -0.263637 -0.456224\n",
      "731        0.0  5.936283  3.604883\n",
      "767        0.0 -0.294432 -0.609920\n",
      "479        0.0 -0.304695 -0.450313\n",
      "747        0.0 -0.356021 -0.615831\n",
      "\n",
      "[800 rows x 3 columns]\n",
      "x\n",
      "96     1\n",
      "262   -1\n",
      "980   -1\n",
      "361   -1\n",
      "328   -1\n",
      "      ..\n",
      "716   -1\n",
      "731   -1\n",
      "767   -1\n",
      "479   -1\n",
      "747   -1\n",
      "Name: target, Length: 800, dtype: int64\n",
      "====== Model summary ======\n",
      "Starting offline clustering...\n",
      "Processed 800 instances.\n",
      "Found 12 clusters offline.\n",
      "\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.merge(anom_scaled_df, normal_scaled_df, how='outer', on=['http_code', 'duration', 'encoding', 'target'])\n",
    "X = df_final.drop(['target'], axis=1)\n",
    "Y = df_final['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "from collections import Counter\n",
    "Counter(y_train)\n",
    "Counter(y_test)\n",
    "\n",
    "print(x_train)\n",
    "print('x')\n",
    "print(y_train)\n",
    "\n",
    "# Log Clustering model testing ===================\n",
    "max_dist = 0.3\n",
    "anomaly_threshold = 0.3\n",
    "model = LogClustering(max_dist=max_dist, anomaly_threshold=anomaly_threshold, mode='offline')\n",
    "model.fit(x_train.to_numpy()) # Use only normal samples for training\n",
    "\n",
    "#print('Train validation:')\n",
    "y_pred_train = model.predict(x_train.to_numpy())\n",
    "print(y_pred_train)\n",
    "#precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "    \n",
    "#print('Test validation:')\n",
    "y_pred_test = model.predict(x_test.to_numpy())\n",
    "print(y_pred_test)\n",
    "#precision, recall, f1 = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89583c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.32      0.81      0.45        58\n",
      "           1       0.78      0.28      0.41       142\n",
      "\n",
      "    accuracy                           0.43       200\n",
      "   macro avg       0.55      0.55      0.43       200\n",
      "weighted avg       0.65      0.43      0.43       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Isolation Forest Model Testing =========\n",
    "x = x_train[y_train == 1]\n",
    "clf = IsolationForest().fit(x)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaac97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Model summary ======\n",
      "Test validation:\n",
      "====== Evaluation summary ======\n",
      "Precision: 0.894, recall: 0.824, F1-measure: 0.857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTree()\n",
    "\n",
    "model.fit(x_train.to_numpy(), y_train)\n",
    "print('Test validation:')\n",
    "precision, recall, f1 = model.evaluate(x_test.to_numpy(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20cebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('isolationforest_duration.sav', 'wb'))\n",
    "pickle.dump(model, open('decisiontree_duration.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63657c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
